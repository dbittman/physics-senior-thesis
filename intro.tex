\section{Introduction}

Boundary value problems, problems defined by a partial differential equation and a set of
constraints on the solution, are found in many areas of physics. These include problems
such as finding the temperature of a rod that has a given temperature on either end,
determining the electric potential inside a region given the potential around the
boundary, or determining the flow of an incompressible fluid. As the complexity of 
these problems increases, however, the feasibility of solving them exactly by hand
quickly decreases. Instead, we have the option to solve them with iterative methods
using computers. These types of problems are well-suited for computers because
they are easily parallelizable, can be discretized easily, and require a lot of repetitive calculation~\cite{parallel}. I have developed an
implementation of such a solver for electrostatics utilizing the 
parallelizability of the problem and designed it to be efficient by taking advantage 
of the design of modern CPUs. It can produce a solution to given problem given a description
of a problem to solve.

This involves having two objectives: ensure that the simulation produces the correct
result, and have it do this quickly. The first of these can be done by comparing the
result to a known solution, and defining a metric for determining how close it is.
This should provide an accurate way to measure how correct the simulation is, allowing
me to compare different methods and ensure that they are all correct.
The second of these objectives can be realized via two methods. Firstly, use an
algorithm that has a reasonably small computational complexity, and secondly,
implement the algorithm in a way that is efficient for modern computers.

\subsection{Poisson's Equation}

Although many differential equations are typically used in boundary value problems,
I will limit the focus to the Poisson equation. Named after SimÃ©on Denis Poisson, the Poisson equation
is used in a number of typical boundary value problems, including electrostatics.
In 2-dimensional Euclidean space it is\cite{boas},
$$\nabla^2 u = f,$$
where $f$ is a real-valued function inside the space and $u$ is the steady-state
solution for the potential in the space. The problems that we are considering are called boundary value problems
because their solutions are dependent both on the value of $f$, but also on the properties of the region and its boundaries.
Formally, the problems to solve are given with $f$ defined and with a region $\Omega$ which has defined boundaries
$\partial\Omega$. These boundary conditions, along with $f$, are the primary inputs to the program which determine
the solution it will give.

\subsection{Discretizing Poisson's Equation}

Since we are solving these problems using a computer, we must transform the problem definition into one which a computer
can actually calculate. Computers do not do well with continuous functions and real numbers. The process of transforming
a problem or equation into one which a computer can handle is called discretization.

\begin{figure}[htb]
	\centering
	\includesvg{grid}
	\caption{Visualization of grid, with indices.}
\label{grid}
\end{figure}

Consider a grid of size $n$ by $n$ as in figure~\ref{grid}\footnote{In general, it is not difficult to allow a rectangular grid, but for simplicity
we will require the grid to be square.}, with values represented by $u_{i,j}$. We want to apply the Poisson equation to
this $u$ as follows:
$$\nabla^2 u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial x^2} = f,$$
however, $u$ is not continuous, so the partial derivative for $x$ becomes
$$\frac{\partial^2 u}{\partial x^2} \approx \frac{u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}}{h^2},$$
at a grid point $(i,j)$, where $h = 1 / n$ is the width and height of a grid cell. The partial derivative for $y$ is similar, using $j$ instead of $i$.
This gives the full discrete Poisson equation as\cite{poisson-relax}\cite{myths},
$$\frac{u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}}{h^2} + \frac{u_{i,j-1} - 2 u_{i,j} + u_{i,j+1}}{h^2} = f_{i,j}.$$

Since $u_{i,j}$ is what we are interested in, this is more useful after some rearranging:
$$h^2f_{i,j} = -4u_{i,j} + u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1},$$
\begin{equation} \label{eq:poisson}
u_{i,j} = \frac{1}{4}(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - h^2f_{i,j}).
\end{equation}

It can be seen from the top equation that this is now a linear algebra problem, requiring a method of solving
$n^2$ dependent linear equations. This means that we can apply one of several standard iterative methods
for solving the problem. This is an excellent application of using a computer to solve numerically a problem
	which is difficult to solve analytically.


\subsection{Jacobi Iteration}
This method is perhaps the simplest method of doing an iterative solution to this linear algebra problem. The procedure
involves treating each equation as independent, and solving iteratively. This turns Equation~\ref{eq:poisson} into\cite{poisson-relax}
\begin{equation} \label{eq:jacobi}
u_{i,j}^{k+1} = \frac{1}{4}(u_{i-1,j}^{k} + u_{i+1,j}^{k} + u_{i,j-1}^{k} + u_{i,j+1}^{k} - h^2f_{i,j}),
\end{equation}
where $k$ is the iteration number. This is saying that to calculate $u$ everywhere, we must start with a guess for $u$,
and then iteratively solve each element in $u$, which depends on its four nearest neighbors. One immediate unfortunate
reality with Jacobi iteration presents itself: we must store a secondary copy of the entire grid because each calculation
of $u_{i,j}^{k+1}$ strictly requires values from the previous iteration. Since we iterate over the grid in order to solve
it, we cannot write the new value for the $k+1$ iteration into that element of $u$, since we would be overwriting a value
that is needed later. This unfortunately but necessarily increases the space required by a factor of $n^2$.

The above equation does not yet include any consideration of boundary conditions. The primary functionality of supporting solving statics problems requires support for dealing with
Dirichlet boundary conditions. Dirichlet boundary conditions are defined such that the solution $\phi$ of a partial
differential equations in a region $\Omega$ has a value $\phi(\mathbf{r}) = y(\mathbf{r}) \; \forall \mathbf{r} \in \partial \Omega$.
For this case, this means that if a Dirichlet boundary value condition is specified for a particular grid cell $(x,y)$,
then its value is simply fixed to the value of the Dirichlet condition, and that cell does not need to be recalculated on
successive iterations.

Another consideration is that the grid is of finite size. This means that when calculating $u_{i=0,j}$, for example, we cannot
incorporate $u_{i-1,j}$ as Equation~\ref{eq:jacobi} would dictate. This is because that would try to get the $-1^{st}$ row the
the matrix $u$, which does not exist. A common method of dealing with this problem is to ``wrap around'' to the other side --
essentially considering the region $\Omega$ to the surface of a sphere, or to consider the space to be periodic. Because of
the nature of the problems that I will be considering, I have chosen to consider an implicit Dirichlet boundary condition
of zero everywhere outside of $\Omega$, thus effectively limiting the calculation to be inside the region while clamping any
values ``outside'' of $\Omega$ to zero. This is easy to implement and visualize as well: any value of $u_{i,j}$ for $i >= n$,
$i < 0$, $j >= n$, or $j < 0$ is simply zero, always.

\subsection{Successive Over-Relaxation}

An alternate approach of iteratively solving this problem numerically is called successive over-relaxation (or SOR), and
it has advantages over simple Jacobi iteration. It works by reusing previous calculations for the next iteration when
calculating a given cell. It transforms Equation~\ref{eq:jacobi} into,
\begin{equation} \label{eq:sor}
u_{i,j}^{k+1} = (1-\omega) + \frac{\omega}{4}\left(u_{i-1,j}^{k+1} + u_{i+1,j}^{k} + u_{i,j-1}^{k+1} + u_{i,j+1}^{k} - h^2 f_{i,j}\right),
\end{equation}
where $\omega$ is a tunable parameter called the \textit{over-relaxation parameter}. For a square lattice, a value of
$$\omega = \frac{2}{1+\frac{\pi}{n}}$$
has the best rate of convergence\cite{poisson-relax}. There are two terms on the right-hand-side of equation~\ref{eq:sor} which refer to the
$k+1$ iteration. What this is doing is using the new values for the cells that have already been updated on this iteration
in order to speed up the convergence.

Successive over-relaxation has a significant advantage over Jacobi iteration, in that it will result in a significantly faster convergence.
These two iterative solutions require iterating over every cell in the grid a number of times. However, the number of iterations needed to
converge is not constant -- it is dependent on the size of the grid as well. For Jacobi iteration, the number of iteration required is
proportional to $n^2$, where $n$ is the length of one side of the grid, thus making the number of iterations $O(n^2)$ and so the total
algorithm $O(n^4)$. SOR beats this by having the number of iterations be proportional to $n$, making SOR an $O(n^3)$ algorithm\cite{poisson-relax}. This means
that SOR is not only significantly faster than Jacobi iteration for large enough $n$, the improvement for SOR over Jacobi iteration increases
as the grid size increases. Furthermore, SOR has an advantage in that it requires less extra space for the computation because it is allowed
to reuse values that have been calculated already in the same iteration, removing the requirement to store old values of cells like in Jacobi
iteration. For this reason I predicted that, while they give the same results eventually, SOR would be a much faster method than Jacobi
iteration in the solving program.






\subsection{Performance Considerations}
While operations on a grid of values are parallelizable, there is a limit to
the effectiveness of multi-threading the algorithm. Take, for example, an average
modern processor with 4 cores each with a 256KB cache and a shared cache of 8MB\@. Each core
can only operate on a grid of 256 by 256 32-bit values before exhausting its cache.
A performance drop will occur if the grid size per core exceeds the core's cache
size. For example, should the grid size exceed 1440 by 1440 32-bit values,
the shared 8MB cache will
be filled, resulting in a massive performance penalty. For this reason, na\"{i}vely splitting
the grid into 4 areas and multi-threading the iteration may result in sub-par performance.
However, like most performance questions, this is difficult to predict.

There are many other aspects of processor design that come into play when writing a program
such as this. Multi-threaded synchronization aside, there are many details of caching that
need to be considered. Of course, optimizing a program is just like any experiment in that
it is absolutely vital to apply normal scientific practices when doing such work. Many make
the mistake of simply writing what they believe to be a more optimized version of a program
without measuring the resulting performance and comparing it to the previous version.

I would expect to see the performance enhanced by the use of multi-threading, but insignificantly
due to the large amount of memory accesses made by the program throughout solving a given problem.
I expect to see a large performance improvement between a na\"{i}ve implementation of the program
and one where memory access patterns have been tuned.


\vspace{10mm}

Using this information, I have written a program which supports both Jacobi iteration and successive over-relaxation,
allowing a varying number of threads to be used, and can calculate statistics about the solution it generates.
It can be given an arbitrary input file to define the parameters, boundary conditions, and initial conditions for
the region it is simulating. This allows me to simulate known problems and compare them to known solutions in order
to verify correctness, and then I will be able
to measure the performance characteristics of different implementations and solution methods with different parameters
and compare them to see which are faster.
