\section{Methods}

Ultimately, the heavy lifting in solving a potential problem involves solving a partial
differential equation -- in this case, Poisson's equation. Here we will be considering
the 2-dimensional Poisson equation in Euclidean space,
$$\nabla^2 \phi = f,$$
where $f$ is a real-valued function inside the space and $\phi$ is the steady-state
solution for the potential in the space. However, since this solution will be calcuated
numerically on a computer, we must discretize the equation so that it can be used to
represent a solution on a 2-dimensional grid.

\subsection{Discreetizing Poisson's Equation}

Consider a grid of size $n$ by $n$\footnote{In general, it is not difficult to allow a rectangular grid, but for simplicity
we will require the grid to be square.}, with values represented by $u_{i,j}$. We want to apply the Poisson equation to
this $u$ as follows:
$$\nabla^2 u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial x^2} = f,$$
however, $u$ is not continuous, so the partial derivative for $x$ becomes
$$\frac{\partial^2 u}{\partial x^2} \approx \frac{u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}}{h^2},$$
at a grid point $(i,j)$, where $h = 1 / n$ is the width and height of a grid cell. The partial derivative for $y$ is similar, using $j$ instead of $i$.
This gives the full discrete Poisson equation as,
$$\frac{u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}}{h^2} + \frac{u_{i,j-1} - 2 u_{i,j} + u_{i,j+1}}{h^2} = f_{i,j}.$$

Since $u_{i,j}$ is what we are interested in, this is more useful after some rearranging:
$$h^2f_{i,j} = -4u_{i,j} + u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1},$$
\begin{equation} \label{eq:poisson}
u_{i,j} = \frac{1}{4}(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - h^2f_{i,j}).
\end{equation}

It can be seen from the top equation that this is now a linear algebra problem, requiring a method of solving
$n^2$ dependent linear equations.

\subsection{Iterative Methods}

Since we are using a computer, an iterative method is feasible to be used as a numerical solution method. In this
program, I have implemented two methods for solving these problems: Jacobi iteration, and successive over-relaxation (SOR).

\subsubsection{Jacobi Iteration}
This method is perhaps the simplest method of doing an iterative solution to this linear algebra problem. The procedure
involves treating each equation as independent, and solving iteratively. This turns Equation~\ref{eq:poisson} into
\begin{equation} \label{eq:jacobi}
u_{i,j}^{k+1} = \frac{1}{4}(u_{i-1,j}^{k} + u_{i+1,j}^{k} + u_{i,j-1}^{k} + u_{i,j+1}^{k} - h^2f_{i,j}),
\end{equation}
where $k$ is the iteration number. This is saying that to calculate $u$ everywhere, we must start with a guess for $u$,
and then iteratively solve each element in $u$, which depends on its four nearest neighbors. One immediate unfortunate
reality with Jacobi iteration presents itself: we must store a secondary copy of the entire grid because each calculation
of $u_{i,j}^{k+1}$ strictly requires values from the previous iteration. Since we iterate over the grid in order to solve
it, we cannot write the new value for the $k+1$ iteration into that element of $u$, since we would be overwriting a value
that is needed later. This unforunately but necessarily increases the space required by a factor of $n^2$.

%TODO: convergence condition?

The above equation does not yet include any consideration of boundary conditions. In fact, this is only valid for a region
that is unbounded. The primary functionality of supporting solving statics problems requires support for dealing with
Dirichlet boundary conditions. Dirichlet boundary conditions are defined such that the solution $\phi$ of a partial
differential equations in a region $\Omega$ has a value $\phi(\mathbf{r}) = y(\mathbf{r}) \; \forall \mathbf{r} \in \partial \Omega$.
For this case, this means that if a Dirichlet boundary value condition is specified for a particular grid cell $(x,y)$,
then its value is simply fixed to the value of the Dirichlet condition, and that cell does not need to be recalculated on
successive iterations.

Another consideration is that the grid is of finite size. This means that when calculating $u_{i=0,j}$, for example, we cannot
incorporate $u{i-1,j}$ as Equation~\ref{eq:jacobi} would dictate. This is because that would try to get the $-1$st row the
the matrix $u$, which does not exist. A common method of dealing with this problem is ``wrap around '' to the other side --
essentially considering the region $\Omega$ to the surface of a sphere, or to consider the space to be periodic. Because of
the nature of the problems that I will be considering, I have chosen to consider an implicit Dirichlet boundary condition
of zero everywhere outside of $\Omega$, thus effectively limiting the calculation to be inside the region while clamping any
values ``outside'' of $\Omega$ to zero. This is easy to implement and visualize as well: any value of $u_{i,j}$ for $i >= n$,
$i < 0$, $j >= n$, or $j < 0$ is simply zero, always.

\subsubsection{Successive Over-Relaxation}

An alternate approach to iteratively solving this problem numerically is called successive over-relaxation (or SOR), and
it has advantages over simple Jacobi iteration. It works by reusing previous calculations for the next iteration when
calculating a given cell. It transforms Equation~\ref{eq:jacobi} into,
\begin{equation} \label{eq:sor}
u_{i,j}^{k+1} = (1-\omega) + \frac{\omega}{4}\left(u_{i-1,j}^{k+1} + u_{i+1,j}^{k} + u_{i,j-1}^{k+1} + u_{i,j+1}^{k} - h^2 f_{i,j}\right),
\end{equation}
where $\omega$ is a tunable parameter called the \textit{over-relaxtion parameter}. For a square lattice, a value of
$$\omega = \frac{2}{1+\frac{\pi}{n}}$$
has the best rate of convergence. There are two terms on the right-hand-side of equation~\ref{eq:sor} which refer to the
$k+1$ iteration. What this is doing is using the new values for the cells that have already been updated on this iteration
in order to speed up the convergence.


TODO Determining when to halt


\subsection{Design of Solver}
The solving program is written in C in order to leverage the full performance of modern computers and is implemented
as a shared library (on \textsc{unix}) such that it can be linked with another program which sets up the initial conditions
before calling the solving function. The C code that does the solving is hereby refered to as the engine, while any code
that interfaces with the engine and provides a usable user interface is referred to as the frontend.

I have written an example frontend in Python, which is capable to parsing a configuration file which describes the
values of $f$, the initial boundary conditions, the grid size, and a number of other details about a given desired
setup (hereby collectively reffered to as a configuration). I have written several example configurations which I have used to
confirm the correctness of the solving program. For a more detailed technical description of the implementation, see Appendix~\ref{app:des}.

\subsection{Configurations and Mapping to Electrostatics}

TODO also talk about how the configuration is written (include advance features like circles)

\subsection{Verifying Correctness}

The clearest way to verify that the solver is working correctly is to test it out on a number of different configurations
for which the correct solution is known. My first choice here was a simple electrostatics problem straight out of a textbook:
%%TODO cite...

\begin{addmargin}[2em]{2em}% 1em left, 2em right
	Consider a square region with the bottom left corner at the origin with each side of length $L$. Let the potential
	along each side of the region be fixed at 0, except for the bottom side, which has a potential $V(x) = V_0 \sin(x \pi / L)$.
	What is the potential $V$ inside the region?
\end{addmargin}

This problem describes solving Laplace's equation (which is Poisson's equation with $f=0$), given some initial boundary conditions.
It can be solved analytically using a standard separation of variables trick. Assume
$$\phi = X(x) Y(y),$$
then
$$X''(x)/X(x) = -Y''(y)/Y(y) = -\lambda,$$
and by applying the boundary conditions, one comes to the solution
$$V(x,y) = \frac{\sinh(y \pi / L)}{\sinh(\pi / L)} \sin(x \pi / L).$$

In order to compare the result of my program with the analytical solution, I added a feature to the Python frontend
to accept a pre-made file containing a precalculated grid of the correct output by using the analytic solution. The
frontend then reads this file and compares it to the result of the solver.
%TODO: measurement of correctness.

\subsection{Potential From Electric Dipole}
Another correctness check is a simulation of an electric dipole. The potential from an electric dipole is well understood.
If two equal and opposite charges are separated on an axis by distance $d$, then at a point $p$ far away, the
potential is,
$$V = C\left(\frac{1}{r_0} - \frac{1}{r_1}\right),$$
where $r_0$ and $r_1$ are the distances from each charge, and $C$ is a constant. With some rearranging,
$$V = C\left(\frac{r_0 - r_1}{r_0 r_1}\right) = \frac{C d \cos(\theta)}{r^2},$$
where $r$ is the distance from the center of charge to the point, and $\theta$ is the angle that the line from the
center of charge forms with the axis that the dipole is on. The potential from an electric dipole falls off as $1/r^2$,
which should be reflected in the solution given by the program if it is given a configuration describing a dipole.

If the solving program is correct, then if two charges are placed near each other, the numerically calculated potential
should display this $1/r^2$ behavior. I wrote such a configuration and had the frontend output the potential at points
increasing in distance from the center of the dipole, allowing me to fit the data to a $1/r^2$ curve.


Correctness Checks
 - Solution to sin(x) along wall potential -- non-discreet.
 - Electric dipole solution for far away

Calculating correctness automatically

\subsection{Optimization and Multithreading}
The purpose of this program is to not only be correct, but to be correct quickly. In order to achieve this, I have
tried three things in combination:
\begin{enumerate}
\item General program optimization, targeted towards 64-bit x86 machines
\item The use of the x86 SIMD instruction set
\item The use of multithreading to split up the work among CPU cores
\end{enumerate}
The first of these is pretty basic. The main part of the solver is written in C so that it can run directly on the
processor. When the program is compiled, I instruct the compiler to optimize the program to the best of its ability,
and the program is written in such a way that it can be optimized well and does not waste time doing unneccesary work.

As an example of the last point, consider a 2-dimensional grid (2-D array), which must be iterated over (much like
this program does). There are two ways to do this: either iterate row-by-row, or column-by-column. The end result is
the same, and one might think the choice is arbitrary. However, when I tested the two different approaches, I found
the row-by-row approach to be around 5 times faster. This is because of caching effects and the way that the CPU
fetches data from RAM. For a more detailed description of this, see Appendix~\ref{app:opt}.

Another example involves data structure organization. Say a program needs to store an array of objects, and each object
has data associated with it (for example, current value, previous value, error, initial state, etc). One might be tempted
to write something like in Listing~\ref{lst:aos}. The problem here is that if each object's \textsc{cur\_val} are all
calculated together (as is often the case, and is definitely the case for this solving program), then the location of
the values that need to be loaded in succession by the processor are far apart. This, again, comes down to caching
effects which are further explained in Appendix~\ref{app:opt}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single,label=lst:aos,caption={Array of structures organization.}]
struct {
	double cur_val, prev_val;
	float err;
	int initial_state;
} objects[N];
\end{lstlisting}
\end{minipage}

Fortunately, this also has an easy to make change that drastically improves performance: change to structure of
arrays organization, thus reorganizing the memory layout so that related and commonly accessed-together values
appear adjacent in memory. This is shown in listing~\ref{lst:soa}. While it may be a less intuitive way to describe
and program a simulation, the result is significantly faster code the majority of the time.

\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single,label=lst:soa,caption={Structure of arrays organization.}]
struct {
	double cur_val[N];
	double prev_val[N];
	float err[N];
	int initial_state[N];
} objects;
\end{lstlisting}
\end{minipage}

\subsection{SIMD Instructions}
I have also made use of the SIMD (Single Instruction Multiple Data) instruction set. These instructions are available
on modern 64-bit x86 processors\footnote{More instructions are added in most new releases of CPUs. There are different
SIMD instruction sets available on different processor models. I have used the sse through avx instruction sets in this
code.}. Their purpose is to enable whats known as vector processing of data on x86 machines. This provides the ability
to apply a mathematical operation to one or more arrays, and have that operation by applied to each element. For example,
I can have a 128-bit register filled with 4 values, $x=(1,2,3,4)$, and other register like it, $y=(10, 10, 11, 11)$. I can
then add them together to get $(11, 12, 14, 15)$, and this addition is issued with a single instruction. Furthermore, this
takes the same amount of time as a single addition, because the processor can do the 4 separate addition operations in
parallel.

I have written in the use of SIMD instructions in the solver program in order to both reduce branches and to parallelize
the calculations required when calculating the next iteration of a given cell. I provided the ability to disable this
functionality and instead do a non-SIMD style calculation in order to get benchmarks. A requirement of the SIMD code was
that it give exactly the same result as the non-SIMD version (otherwise the implementation of the math would be incorrect).
For this reason, when talking about the results of a simulation, it does not matter if SIMD was used or not; the use
of SIMD only affects performance.

An important note is that when fully optimizing code for a machine which it knows about, \texttt{gcc} may choose to output
SIMD instructions in order to do computation even if the user does not do so explicitly. For this reason, in places where
I refer to the mode of the simulation as being ``non-SIMD'', what I mean is that it does not use my manual SIMD instructions
but it may still use compiler-generated SIMD instructions.

\subsection{Multithreading}
Finally, I have added multithreading support to the solver. This is done by spitting up the grid into subregions which
are contiguous in memory, and spawning $N$ threads via the pthreads API. Each thread is then given a region to work on,
and they coordinate through shared state in order to decide when the solution has been reached. The initial implementation
showed extremely poor results with multithreading (often being drastically slowed than the single threaded version)
due to the use of atomic shared variables. It was then re-written such that the code was unsafe but much faster. This
is because each thread modifies a shared non-atomic variable that keeps track of the current total change per iteration
after that thread itself completes a number of iterations over its region. This kind of shared memory access is much faster
than coordinating the threads, but it is also technically undefined behavior in C. Fortunately, on x86, it is actually
safe, however there is potential information loss because a thread could overwrite another thread if they are trying to
update the value at the same time. A more detailed discussion of the code and why it is unsafe can be found in Appendix~\ref{app:opt}.

\subsection{Benchmarking Performance}
Benchmarks were done by placing two calls to \texttt{gettimeofday(3)} around the core of the solver. This function has a
granularity of 1 microsecond CITE ME. This code returns
both the number of iterations done and the difference between the two gettimeofday calls, which are then used to calculate
the iterations per second. In order to take into account error from unpredicatble scheduling effect from the operating system,
each benchmark was run 300 times, and the standard deviation and mean were calculated, along with 95\% confidence intervals. Several different computers were used:
computer A was an Intel Core i5-3570K with 4 cores running at 3.4GHz with 16GB of RAM. Computer B was an Intel Xeon E5620
with 4 cores with 2 threads each, running at 2.4GHz, with 24GB of RAM. Computer B did not support all of the SIMD instructions
that I used, so it was only able to benchmark the non-SIMD version of the code.

